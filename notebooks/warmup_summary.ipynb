{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0058d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import hmean\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pal = sns.color_palette(\"deep\", 10)\n",
    "model_colors = {\n",
    "    \"Ablation\": pal[7],\n",
    "    \"ICL-FT\": pal[8],\n",
    "    \"llama3_ft\": pal[8],\n",
    "    \"RAG-FT\": pal[5],\n",
    "    \"rag_trained\": pal[6],\n",
    "    \"MemLLM\": pal[9],\n",
    "    \"GNM\": pal[0],\n",
    "    \"gnm\": pal[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_means(A):\n",
    "    A = np.asarray(A)\n",
    "    n, m = A.shape\n",
    "    assert n == m, \"expect square matrix\"\n",
    "    return np.array([A.diagonal(offset=k).mean() for k in range(-(n-1), n)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805793aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"naive_model\": \"Base\",\n",
    "    # \"llama3\": \"ICL\",\n",
    "    \"llama3_ft\": \"ICL-FT\",\n",
    "    # \"rag_untrained\": \"RAG\",\n",
    "    \"rag_trained\": \"RAG-FT\",\n",
    "    \"memoryllm\": \"MemLLM\",\n",
    "    \"gnm\": \"GNM\",\n",
    "}\n",
    "\n",
    "model_keys = list(models.keys())\n",
    "model_names = list(models.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ba2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_colors = {\n",
    "#     \"Base\": pal[3],\n",
    "#     \"ICL-FT\": pal[1],\n",
    "#     \"RAG-FT\": pal[2],\n",
    "#     \"MemLLM\": pal[4],\n",
    "#     \"GNM\": pal[0],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keys, model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21869719",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "\n",
    "for model_key, model_name in models.items():\n",
    "\n",
    "    data_root = f\"../saved_evals/warmup_seq_10/{model_key}/summary.json\"\n",
    "\n",
    "    # Open the file and load the data\n",
    "    with open(data_root, 'r') as json_file:\n",
    "        data_dict = json.load(json_file)\n",
    "\n",
    "    data[model_key] = data_dict\n",
    "    data[model_key][\"model_name\"] = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_rollout = 10\n",
    "# step_0 = len_rollout-1\n",
    "\n",
    "# old_measure_step = 5\n",
    "\n",
    "rows = []\n",
    "\n",
    "for k, v in data.items():\n",
    "\n",
    "\n",
    "    fa_mat = np.array(v[\"fact_accuracy_matrix\"])[0]\n",
    "    all_acc = np.mean(fa_mat[np.triu_indices_from(fa_mat, k=0)])\n",
    "    \n",
    "\n",
    "    # f_acc_diags = diag_means(fa_mat)[step_0:]\n",
    "\n",
    "    fs_mat = np.array(v[\"fact_specificity_matrix\"])[0]\n",
    "    all_spec = np.mean(fs_mat[np.triu_indices_from(fs_mat, k=0)])\n",
    "    \n",
    "\n",
    "    # f_spec_diags = diag_means(fs_mat)[step_0:]\n",
    "\n",
    "    fs_mat = np.array(v[\"fact_selectivity_matrix\"])[0]\n",
    "    all_sel = np.mean(fs_mat[np.triu_indices_from(fs_mat, k=0)])\n",
    "    \n",
    "\n",
    "    # f_sel_diags = diag_means(fs_mat)[step_0:]\n",
    "    \n",
    "    total_all_score = harmonic_mean = hmean(\n",
    "        np.array(\n",
    "            [\n",
    "                all_acc,\n",
    "                all_spec,\n",
    "                all_sel\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # f_score_diags = np.mean(np.vstack([f_acc_diags, f_spec_diags, f_sel_diags]), 0)\n",
    "\n",
    "    # Extract fact_retention and fact_accuracy_over_time arrays\n",
    "    fact_retention = v.get(\"fact_retention\", [0] * 10)  # Default to zeros if not found\n",
    "    fact_accuracy_over_time = v.get(\"fact_accuracy_over_time\", [0] * 10)  # Default to zeros if not found\n",
    "    \n",
    "    scale = 100\n",
    "\n",
    "    row = [\n",
    "        # k,\n",
    "        v[\"model_name\"],\n",
    "        total_all_score*scale,\n",
    "        all_acc*scale,\n",
    "        all_spec*scale,\n",
    "        all_sel*scale,\n",
    "        # Add fact_retention values (assuming they're percentages or 0-1 values)\n",
    "        np.mean(fact_retention)*scale if len(fact_retention) > 0 else 0,\n",
    "        # Add fact_accuracy_over_time values\n",
    "        np.mean(fact_accuracy_over_time)*scale if len(fact_accuracy_over_time) > 0 else 0,\n",
    "        # f_score_diags[0]*scale,\n",
    "        # f_score_diags[old_measure_step]*scale\n",
    "    ]\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "\n",
    "columns = [\n",
    "    \"model_name\",\n",
    "    \"total_all_score\",\n",
    "    \"all_fact_accuracy\",\n",
    "    \"all_fact_specificity\",\n",
    "    \"all_fact_selectivity\",\n",
    "    \"fact_retention\",\n",
    "    \"fact_accuracy_over_time\",\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(rows, columns=columns)\n",
    "results_df.round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bd0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31779d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models = {\n",
    "    \"naive_model\": \"Base\",\n",
    "    \"memoryllm\": \"MemLLM\",\n",
    "    \"llama3_ft\": \"ICL-FT\",\n",
    "    \"rag_trained\": \"RAG-FT\",\n",
    "    \"gnm\": \"GNM\",\n",
    "}\n",
    "\n",
    "plot_model_names = list(plot_models.keys())\n",
    "\n",
    "plot_cols_1 = [\n",
    "    'total_all_score', \n",
    "    'all_fact_accuracy',\n",
    "    'all_fact_specificity', \n",
    "    'all_fact_selectivity', \n",
    "]\n",
    "pc1_names = [\n",
    "    \"Score\", \"Accuracy\", \"Specificity\", \"Selectivity\"\n",
    "]\n",
    "\n",
    "plot_cols_2 = [\n",
    "    'fact_retention',\n",
    "]\n",
    "pc2_names = [\n",
    "    \"Retention\"\n",
    "]\n",
    "\n",
    "plot_cols_3 = [\n",
    "    'fact_accuracy_over_time',\n",
    "]\n",
    "pc3_names = [\n",
    "    \"Accuracy\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- choose the model display order / labels ---\n",
    "model_order = list(plot_models.values())\n",
    "model_order_bars = [m for m in model_order if m != \"Base\"]\n",
    "\n",
    "\n",
    "def grouped_bar_panel(ax, df, metrics, model_order, model_order_bars, tick_names, title=None, ylim=None, show_base_lines=True):\n",
    "    \"\"\"\n",
    "    One axis: x = metrics, within each metric: bars for each model.\n",
    "    Base model shown as horizontal dotted lines.\n",
    "    \"\"\"\n",
    "    # keep only the models you want, and order them\n",
    "    sub = df[df[\"model_name\"].isin(model_order)].copy()\n",
    "    sub = sub.set_index(\"model_name\").reindex(model_order)\n",
    "\n",
    "    n_models = len(model_order_bars)\n",
    "    n_metrics = len(metrics)\n",
    "\n",
    "    x = np.arange(n_metrics)                       # metric group centers\n",
    "    group_width = 0.8                              # total width reserved per metric group\n",
    "    bar_w = group_width / n_models\n",
    "\n",
    "    # Draw bars for non-Base models with opacity for all except GNM\n",
    "    for i, m in enumerate(model_order_bars):\n",
    "        offsets = x - group_width/2 + (i + 0.5)*bar_w\n",
    "        # Use full opacity for GNM, reduced for others\n",
    "        alpha = 1.0 if m == \"GNM\" else 1.0\n",
    "        ax.bar(offsets, sub.loc[m, metrics].values, width=bar_w, label=m, \n",
    "               color=model_colors[m], alpha=alpha)\n",
    "\n",
    "    # Draw horizontal dashed lines for Base model\n",
    "    if show_base_lines:\n",
    "        base_values = sub.loc[\"Base\", metrics].values\n",
    "        for i, val in enumerate(base_values):\n",
    "            # Calculate span from left edge of first bar to right edge of last bar in data coordinates\n",
    "            x_left = i - group_width/2\n",
    "            x_right = i + group_width/2\n",
    "            ax.plot([x_left, x_right], [val, val], \n",
    "                    color='black', linestyle='--', linewidth=2, dashes=(5, 2))\n",
    "        \n",
    "        # Add Base to legend (create a dummy line)\n",
    "        ax.plot([], [], color='black', linestyle='--', linewidth=2, dashes=(5, 2), label=\"Base\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(tick_names, size=16)\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=16)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "\n",
    "\n",
    "def line_chart_panel(ax, data_dict, metric_name, model_order, title=None, ylim=None, xlabel=\"Steps Since Seen\"):\n",
    "    \"\"\"\n",
    "    Line chart showing metric values over time steps for each model.\n",
    "    \"\"\"\n",
    "    # Plot non-Base models\n",
    "    for model_key, model_vals in data_dict.items():\n",
    "        model_name = model_vals[\"model_name\"]\n",
    "        if model_name not in model_order:\n",
    "            continue\n",
    "        \n",
    "        # Get the metric array (nested in a list)\n",
    "        metric_values = np.array(model_vals.get(metric_name, [[]])[0]) * 100\n",
    "        \n",
    "        if len(metric_values) == 0:\n",
    "            continue\n",
    "            \n",
    "        x = np.arange(len(metric_values))\n",
    "        \n",
    "        if model_name == \"Base\":\n",
    "            # Dashed line for Base\n",
    "            ax.plot(x, metric_values, \"--o\", label=model_name, \n",
    "                   color='black', linewidth=2, markersize=6, dashes=(5, 2))\n",
    "        else:\n",
    "            # Solid lines for other models with opacity\n",
    "            alpha = 1.0 if model_name == \"GNM\" else 1.0\n",
    "            ax.plot(x, metric_values, \"-o\", label=model_name, \n",
    "                   color=model_colors[model_name], linewidth=2, markersize=6, alpha=alpha)\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=16)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "    \n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "\n",
    "# --- build the figure ---\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(\n",
    "    1, 3, \n",
    "    width_ratios=(0.5, 0.25, 0.25),\n",
    "    figsize=(13, 3), sharey=False, constrained_layout=True\n",
    ")\n",
    "\n",
    "grouped_bar_panel(\n",
    "    ax1,\n",
    "    results_df,\n",
    "    plot_cols_1,\n",
    "    model_order,\n",
    "    model_order_bars,\n",
    "    pc1_names,\n",
    "    title=\"Performance Over All Time Steps\",\n",
    "    ylim=(0, 100)\n",
    ")\n",
    "\n",
    "line_chart_panel(\n",
    "    ax2,\n",
    "    data,\n",
    "    \"fact_retention\",\n",
    "    model_order,\n",
    "    title=\"Retention\",\n",
    "    ylim=(15, 100),\n",
    "    xlabel=\"Steps Since Seen\"\n",
    ")\n",
    "\n",
    "line_chart_panel(\n",
    "    ax3,\n",
    "    data,\n",
    "    \"fact_accuracy_over_time\",\n",
    "    model_order,\n",
    "    title=\"Accuracy Stability\",\n",
    "    ylim=(70, 100),\n",
    "    xlabel=\"Total Documents Seen\"\n",
    ")\n",
    "\n",
    "ax1.set_ylabel(\"Percent (%)\", fontsize=14)\n",
    "ax2.set_ylabel(\"Percent (%)\", fontsize=14)\n",
    "\n",
    "# legend once (outside) - Base will now be included with dotted line\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(0.16, -0.04), ncol=5, fontsize=16)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(\"../plots/warmup_bar.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative version using harmonic mean of all three metrics (accuracy, specificity, selectivity)\n",
    "\n",
    "# First, compute the over_time and retention metrics for all three metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import hmean\n",
    "\n",
    "# Extract metrics for each model\n",
    "retention_data = {}\n",
    "over_time_data = {}\n",
    "\n",
    "for model_key, model_vals in data.items():\n",
    "    model_name = model_vals[\"model_name\"]\n",
    "    \n",
    "    # Extract fact_accuracy_over_time (already exists)\n",
    "    accuracy_over_time = np.array(model_vals.get(\"fact_accuracy_over_time\", [[]])[0])\n",
    "    \n",
    "    # Extract specificity_over_time from diagonal of fact_specificity_matrix\n",
    "    specificity_matrix = np.array(model_vals[\"fact_specificity_matrix\"])[0]\n",
    "    specificity_over_time = np.diag(specificity_matrix)\n",
    "    \n",
    "    # Extract selectivity_over_time from diagonal of fact_selectivity_matrix\n",
    "    selectivity_matrix = np.array(model_vals[\"fact_selectivity_matrix\"])[0]\n",
    "    selectivity_over_time = np.diag(selectivity_matrix)\n",
    "    \n",
    "    # Extract fact_retention (already exists)\n",
    "    accuracy_retention = np.array(model_vals.get(\"fact_retention\", [[]])[0])\n",
    "    \n",
    "    # Compute specificity_retention (average along diagonal offsets)\n",
    "    specificity_retention = diag_means(specificity_matrix)[len(specificity_matrix)-1:]\n",
    "    \n",
    "    # Compute selectivity_retention (average along diagonal offsets)\n",
    "    selectivity_retention = diag_means(selectivity_matrix)[len(selectivity_matrix)-1:]\n",
    "    \n",
    "    # Compute harmonic means\n",
    "    # For over_time metrics\n",
    "    over_time_hmean = []\n",
    "    for i in range(len(accuracy_over_time)):\n",
    "        over_time_hmean.append(hmean([\n",
    "            accuracy_over_time[i],\n",
    "            specificity_over_time[i],\n",
    "            selectivity_over_time[i]\n",
    "        ]))\n",
    "    \n",
    "    # For retention metrics\n",
    "    retention_hmean = []\n",
    "    for i in range(len(accuracy_retention)):\n",
    "        retention_hmean.append(hmean([\n",
    "            accuracy_retention[i],\n",
    "            specificity_retention[i],\n",
    "            selectivity_retention[i]\n",
    "        ]))\n",
    "    \n",
    "    retention_data[model_key] = {\n",
    "        \"model_name\": model_name,\n",
    "        \"values\": np.array(retention_hmean)\n",
    "    }\n",
    "    \n",
    "    over_time_data[model_key] = {\n",
    "        \"model_name\": model_name,\n",
    "        \"values\": np.array(over_time_hmean)\n",
    "    }\n",
    "\n",
    "# --- build the figure ---\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(\n",
    "    1, 3, \n",
    "    width_ratios=(0.5, 0.25, 0.25),\n",
    "    figsize=(13, 3), sharey=False, constrained_layout=True\n",
    ")\n",
    "\n",
    "# First panel: Same bar chart as before\n",
    "grouped_bar_panel(\n",
    "    ax1,\n",
    "    results_df,\n",
    "    plot_cols_1,\n",
    "    model_order,\n",
    "    model_order_bars,\n",
    "    pc1_names,\n",
    "    title=\"Performance Over All Time Steps\",\n",
    "    ylim=(0, 100)\n",
    ")\n",
    "\n",
    "# Second panel: Line chart of retention (harmonic mean)\n",
    "for model_key, metric_data in retention_data.items():\n",
    "    model_name = metric_data[\"model_name\"]\n",
    "    if model_name not in model_order:\n",
    "        continue\n",
    "    \n",
    "    metric_values = metric_data[\"values\"] * 100\n",
    "    x = np.arange(len(metric_values))\n",
    "    \n",
    "    if model_name == \"Base\":\n",
    "        ax2.plot(x, metric_values, \"--o\", label=model_name, \n",
    "                color='black', linewidth=2, markersize=6, dashes=(5, 2))\n",
    "    else:\n",
    "        alpha = 1.0 if model_name == \"GNM\" else 0.7\n",
    "        ax2.plot(x, metric_values, \"-o\", label=model_name, \n",
    "                color=model_colors[model_name], linewidth=2, markersize=6, alpha=alpha)\n",
    "\n",
    "ax2.set_title(\"Score Retention\", fontsize=16)\n",
    "ax2.set_xlabel(\"Steps Since Seen\", fontsize=14)\n",
    "ax2.set_ylabel(\"Percent (%)\", fontsize=14)\n",
    "ax2.set_xticks(x)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim(30, 100)\n",
    "\n",
    "# Third panel: Line chart of over_time (harmonic mean)\n",
    "for model_key, metric_data in over_time_data.items():\n",
    "    model_name = metric_data[\"model_name\"]\n",
    "    if model_name not in model_order:\n",
    "        continue\n",
    "    \n",
    "    metric_values = metric_data[\"values\"] * 100\n",
    "    x = np.arange(len(metric_values))\n",
    "    \n",
    "    if model_name == \"Base\":\n",
    "        ax3.plot(x, metric_values, \"--o\", label=model_name, \n",
    "                color='black', linewidth=2, markersize=6, dashes=(5, 2))\n",
    "    else:\n",
    "        alpha = 1.0 if model_name == \"GNM\" else 0.7\n",
    "        ax3.plot(x, metric_values, \"-o\", label=model_name, \n",
    "                color=model_colors[model_name], linewidth=2, markersize=6, alpha=alpha)\n",
    "\n",
    "ax3.set_title(\"Score Stability\", fontsize=16)\n",
    "ax3.set_xlabel(\"Documents Seen\", fontsize=14)\n",
    "ax3.set_xticks(x)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "ax3.set_ylim(30, 100)\n",
    "\n",
    "ax1.set_ylabel(\"Percent (%)\", fontsize=14)\n",
    "\n",
    "# legend once (outside)\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(0.16, -0.04), ncol=5, fontsize=16)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(\"../plots/warmup_bar_hmean.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc149da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third version: Bar chart showing specific time steps from retention data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract specific time steps from retention_data (indices 0, 3, 7)\n",
    "retention_snapshots = {}\n",
    "for model_key, metric_data in retention_data.items():\n",
    "    model_name = metric_data[\"model_name\"]\n",
    "    values = metric_data[\"values\"] * 100\n",
    "    \n",
    "    # Extract values at indices 0, 3, and 7\n",
    "    retention_snapshots[model_name] = {\n",
    "        \"most_recent\": values[0] if len(values) > 0 else 0,\n",
    "        \"4_steps_ago\": values[4] if len(values) > 4 else 0,\n",
    "        \"8_steps_ago\": values[8] if len(values) > 8 else 0,\n",
    "    }\n",
    "\n",
    "# Create DataFrame for the snapshot data\n",
    "snapshot_rows = []\n",
    "for model_name in model_order:\n",
    "    if model_name in retention_snapshots:\n",
    "        snapshot_rows.append([\n",
    "            model_name,\n",
    "            retention_snapshots[model_name][\"most_recent\"],\n",
    "            retention_snapshots[model_name][\"4_steps_ago\"],\n",
    "            retention_snapshots[model_name][\"8_steps_ago\"],\n",
    "        ])\n",
    "\n",
    "snapshot_df = pd.DataFrame(snapshot_rows, columns=[\"model_name\", \"most_recent\", \"4_steps_ago\", \"8_steps_ago\"])\n",
    "# --- build the figure ---\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    1, 2, \n",
    "    width_ratios=(0.6, 0.4),\n",
    "    figsize=(13, 3), sharey=False, constrained_layout=True\n",
    ")\n",
    "\n",
    "# First panel: Same bar chart as before\n",
    "grouped_bar_panel(\n",
    "    ax1,\n",
    "    results_df,\n",
    "    plot_cols_1,\n",
    "    model_order,\n",
    "    model_order_bars,\n",
    "    pc1_names,\n",
    "    title=\"Total Performance\",\n",
    "    ylim=(0, 100)\n",
    ")\n",
    "\n",
    "# Second panel: Bar chart with retention at specific time steps\n",
    "grouped_bar_panel(\n",
    "    ax2,\n",
    "    snapshot_df,\n",
    "    [\"most_recent\", \"4_steps_ago\", \"8_steps_ago\"],\n",
    "    model_order,\n",
    "    model_order_bars,\n",
    "    [\"Most Recent\", \"4 Steps Ago\", \"8 Steps Ago\"],\n",
    "    title=\"Performance By Recency\",\n",
    "    ylim=(0, 100),\n",
    "    show_base_lines=False\n",
    ")\n",
    "\n",
    "ax1.set_ylabel(\"Percent (%)\", fontsize=14)\n",
    "# ax2.set_ylabel(\"Percent (%)\", fontsize=14)\n",
    "\n",
    "# legend once (outside)\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(0.24, -0.04), ncol=5, fontsize=16)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(\"../plots/warmup_bar_snapshots.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third version: Bar chart showing specific time steps from retention data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Binomial CI function\n",
    "def binomial_ci(p, n, z=1.96):\n",
    "    \"\"\"Returns 95% CI half-width for a proportion.\"\"\"\n",
    "    se = np.sqrt(p * (1 - p) / n)\n",
    "    return z * se\n",
    "\n",
    "# Sample counts\n",
    "n_episodes = 216\n",
    "n_upper_triangle = 55  # 10+9+8+...+1 for 10x10 matrix\n",
    "n_total_performance = n_episodes * n_upper_triangle  # 11880\n",
    "\n",
    "# Sample counts for retention snapshots (index 0, 4, 9)\n",
    "n_samples_retention = {\n",
    "    \"most_recent\": 10 * n_episodes,   # 2160\n",
    "    \"5_steps_ago\": 6 * n_episodes,    # 1296\n",
    "    \"10_steps_ago\": 1 * n_episodes,   # 216\n",
    "}\n",
    "\n",
    "# Extract specific time steps from retention_data (indices 0, 4, 9)\n",
    "retention_snapshots = {}\n",
    "for model_key, metric_data in retention_data.items():\n",
    "    model_name = metric_data[\"model_name\"]\n",
    "    values = metric_data[\"values\"] * 100\n",
    "    \n",
    "    # Extract values at indices 0, 4, and 9\n",
    "    retention_snapshots[model_name] = {\n",
    "        \"most_recent\": values[0] if len(values) > 0 else 0,\n",
    "        \"5_steps_ago\": values[4] if len(values) > 4 else 0,\n",
    "        \"10_steps_ago\": values[9] if len(values) > 9 else 0,\n",
    "    }\n",
    "\n",
    "# Create DataFrame for the snapshot data\n",
    "snapshot_rows = []\n",
    "for model_name in model_order:\n",
    "    if model_name in retention_snapshots:\n",
    "        snapshot_rows.append([\n",
    "            model_name,\n",
    "            retention_snapshots[model_name][\"most_recent\"],\n",
    "            retention_snapshots[model_name][\"5_steps_ago\"],\n",
    "            retention_snapshots[model_name][\"10_steps_ago\"],\n",
    "        ])\n",
    "\n",
    "snapshot_df = pd.DataFrame(snapshot_rows, columns=[\"model_name\", \"most_recent\", \"5_steps_ago\", \"10_steps_ago\"])\n",
    "\n",
    "# Modified grouped_bar_panel with error bars\n",
    "def grouped_bar_panel_with_ci(ax, df, metrics, model_order, model_order_bars, tick_names, \n",
    "                               n_samples_dict, title=None, ylim=None, show_base_lines=True):\n",
    "    \"\"\"\n",
    "    One axis: x = metrics, within each metric: bars for each model.\n",
    "    Base model shown as horizontal dotted lines. Includes 95% CI error bars.\n",
    "    n_samples_dict: dict mapping metric name to sample count, or single int for all metrics\n",
    "    \"\"\"\n",
    "    # keep only the models you want, and order them\n",
    "    sub = df[df[\"model_name\"].isin(model_order)].copy()\n",
    "    sub = sub.set_index(\"model_name\").reindex(model_order)\n",
    "\n",
    "    n_models = len(model_order_bars)\n",
    "    n_metrics = len(metrics)\n",
    "\n",
    "    x = np.arange(n_metrics)\n",
    "    group_width = 0.8\n",
    "    bar_w = group_width / n_models\n",
    "\n",
    "    # Draw bars for non-Base models with error bars\n",
    "    for i, m in enumerate(model_order_bars):\n",
    "        offsets = x - group_width/2 + (i + 0.5)*bar_w\n",
    "        values = sub.loc[m, metrics].values\n",
    "        \n",
    "        # Compute CIs for each metric\n",
    "        cis = []\n",
    "        for j, metric in enumerate(metrics):\n",
    "            if isinstance(n_samples_dict, dict):\n",
    "                n = n_samples_dict[metric]\n",
    "            else:\n",
    "                n = n_samples_dict\n",
    "            # Convert percentage back to proportion for CI calculation\n",
    "            p = values[j] / 100\n",
    "            ci = binomial_ci(p, n) * 100  # Convert CI back to percentage\n",
    "            cis.append(ci)\n",
    "        \n",
    "        ax.bar(offsets, values, width=bar_w, label=m, \n",
    "               color=model_colors[m], alpha=1.0, yerr=cis, capsize=0, \n",
    "               error_kw={'linewidth': 1.5})\n",
    "\n",
    "    # Draw horizontal dashed lines for Base model\n",
    "    if show_base_lines:\n",
    "        base_values = sub.loc[\"Base\", metrics].values\n",
    "        for i, val in enumerate(base_values):\n",
    "            x_left = i - group_width/2\n",
    "            x_right = i + group_width/2\n",
    "            ax.plot([x_left, x_right], [val, val], \n",
    "                    color='black', linestyle='--', linewidth=2, dashes=(5, 2))\n",
    "        \n",
    "        ax.plot([], [], color='black', linestyle='--', linewidth=2, dashes=(5, 2), label=\"Base\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(tick_names, size=16)\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=16)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "\n",
    "# --- build the figure ---\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    1, 2, \n",
    "    width_ratios=(0.6, 0.4),\n",
    "    figsize=(13, 2.5), sharey=False, constrained_layout=True\n",
    ")\n",
    "\n",
    "# First panel: Bar chart with CI (all metrics use n_total_performance)\n",
    "grouped_bar_panel_with_ci(\n",
    "    ax1,\n",
    "    results_df,\n",
    "    plot_cols_1,\n",
    "    model_order,\n",
    "    model_order_bars,\n",
    "    pc1_names,\n",
    "    n_samples_dict=n_total_performance,  # 11880 samples\n",
    "    title=\"(C) Total Performance\",\n",
    "    ylim=(0, 100)\n",
    ")\n",
    "\n",
    "# Second panel: Bar chart with retention at specific time steps (different n per metric)\n",
    "grouped_bar_panel_with_ci(\n",
    "    ax2,\n",
    "    snapshot_df,\n",
    "    [\"most_recent\", \"5_steps_ago\", \"10_steps_ago\"],\n",
    "    model_order,\n",
    "    model_order_bars,\n",
    "    [\"Most Recent\", \"4 Steps Ago\", \"9 Steps Ago\"],\n",
    "    n_samples_dict=n_samples_retention,  # {2160, 1296, 216}\n",
    "    title=\"(D) Performance By Recency\",\n",
    "    ylim=(0, 100),\n",
    "    show_base_lines=False\n",
    ")\n",
    "\n",
    "ax1.set_ylabel(\"Percent (%)\", fontsize=14)\n",
    "\n",
    "# legend on the right side, horizontally stacked\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(1.0, 0.5), ncol=1, fontsize=14)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(\"../plots/warmup_bar_snapshots_10.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0155526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845157b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- choose the model display order / labels ---\n",
    "# # If results_df[\"model_name\"] already contains display names like \"Base\", \"MetaICL\", etc:\n",
    "# model_order = list(plot_models.values())  # [\"Base\",\"MetaICL\",\"RAG-FT\",\"MemLLM\",\"Ours\"] in your chosen order\n",
    "# # If instead results_df uses internal keys like \"naive_model\", then swap to:\n",
    "# # model_order = list(plot_models.keys())\n",
    "# model_order_bars = [m for m in model_order if m != \"Base\"]\n",
    "\n",
    "# fig, (ax_left, ax_right) = plt.subplots(\n",
    "#     1, 2, \n",
    "#     width_ratios=(0.7, 0.3),\n",
    "#     figsize=(13, 3), \n",
    "#     sharey=False, \n",
    "#     constrained_layout=True\n",
    "# )\n",
    "\n",
    "# # Left panel: Bar chart for metrics averaged across time steps 0-7\n",
    "# # This should match averaging the line values on the right panel\n",
    "# rows_8steps = []\n",
    "# curve_len_8 = 10\n",
    "\n",
    "# for k, v in data.items():\n",
    "#     # Use full matrix and compute step_0_offset dynamically\n",
    "#     fa_mat = np.array(v[\"fact_accuracy_matrix\"])[0]\n",
    "#     mat_size = fa_mat.shape[0]\n",
    "#     step_0_offset = mat_size - 1\n",
    "#     # Get diagonal means for each time step (0-7 steps ago)\n",
    "#     f_acc_diags = diag_means(fa_mat)[step_0_offset:step_0_offset+curve_len_8]\n",
    "#     all_acc_8 = np.mean(f_acc_diags)  # Average across the 8 time steps\n",
    "    \n",
    "#     fs_mat = np.array(v[\"fact_specificity_matrix\"])[0]\n",
    "#     f_spec_diags = diag_means(fs_mat)[step_0_offset:step_0_offset+curve_len_8]\n",
    "#     all_spec_8 = np.mean(f_spec_diags)\n",
    "    \n",
    "#     fsel_mat = np.array(v[\"fact_selectivity_matrix\"])[0]\n",
    "#     f_sel_diags = diag_means(fsel_mat)[step_0_offset:step_0_offset+curve_len_8]\n",
    "#     all_sel_8 = np.mean(f_sel_diags)\n",
    "    \n",
    "#     # Score is the average of the three metrics (same as on right panel)\n",
    "#     metric_values = np.mean(np.vstack([f_acc_diags, f_spec_diags, f_sel_diags]), 0)\n",
    "#     total_score_8 = np.mean(metric_values)  # Average across the 8 time steps\n",
    "    \n",
    "#     row_8 = [\n",
    "#         v[\"model_name\"],\n",
    "#         total_score_8 * 100,\n",
    "#         all_acc_8 * 100,\n",
    "#         all_spec_8 * 100,\n",
    "#         all_sel_8 * 100,\n",
    "#     ]\n",
    "#     rows_8steps.append(row_8)\n",
    "\n",
    "# results_df_8 = pd.DataFrame(rows_8steps, columns=['model_name', 'Score', 'Accuracy', 'Specificity', 'Selectivity'])\n",
    "\n",
    "# # Plot bars on left panel\n",
    "# metrics_8 = ['Score', 'Accuracy', 'Specificity', 'Selectivity']\n",
    "# sub = results_df_8[results_df_8[\"model_name\"].isin(model_order)].copy()\n",
    "# sub = sub.set_index(\"model_name\").reindex(model_order)\n",
    "\n",
    "# n_models = len(model_order_bars)\n",
    "# n_metrics = len(metrics_8)\n",
    "\n",
    "# x_bar = np.arange(n_metrics)\n",
    "# group_width = 0.8\n",
    "# bar_w = group_width / n_models\n",
    "\n",
    "# for i, m in enumerate(model_order_bars):\n",
    "#     offsets = x_bar - group_width/2 + (i + 0.5)*bar_w\n",
    "#     alpha = 1.0 if m == \"GNM\" else 0.7\n",
    "#     ax_left.bar(offsets, sub.loc[m, metrics_8].values, width=bar_w, label=m, \n",
    "#                 color=model_colors[m], alpha=alpha)\n",
    "\n",
    "# # Draw horizontal dashed lines for Base model\n",
    "# base_values_8 = sub.loc[\"Base\", metrics_8].values\n",
    "# for i, val in enumerate(base_values_8):\n",
    "#     x_left = i - group_width/2\n",
    "#     x_right = i + group_width/2\n",
    "#     ax_left.plot([x_left, x_right], [val, val], \n",
    "#                  color='black', linestyle='--', linewidth=2, dashes=(5, 2))\n",
    "\n",
    "# ax_left.plot([], [], color='black', linestyle='--', linewidth=2, dashes=(5, 2), label=\"Base\")\n",
    "\n",
    "# ax_left.set_xticks(x_bar)\n",
    "# ax_left.set_xticklabels(metrics_8, size=16)\n",
    "# ax_left.set_title(\"Performance Across 8 Time Steps\", fontsize=16)\n",
    "# ax_left.set_ylim(0, 100)\n",
    "# ax_left.set_ylabel(\"Percent (%)\", fontsize=14)\n",
    "\n",
    "# # Right panel: Line chart showing score over time\n",
    "# x_line = np.arange(curve_len_8)\n",
    "\n",
    "# # Plot non-Base models first (to match legend order from bars)\n",
    "# for model_name in model_order_bars:\n",
    "#     # Find the corresponding key in data\n",
    "#     k = [key for key, val in data.items() if val[\"model_name\"] == model_name][0]\n",
    "#     v = data[k]\n",
    "    \n",
    "#     fa_mat = np.array(v[\"fact_accuracy_matrix\"])[0]\n",
    "#     mat_size = fa_mat.shape[0]\n",
    "#     step_0_offset = mat_size - 1\n",
    "#     f_acc_diags = diag_means(fa_mat)[step_0_offset:step_0_offset+curve_len_8]\n",
    "    \n",
    "#     fs_mat = np.array(v[\"fact_specificity_matrix\"])[0]\n",
    "#     f_spec_diags = diag_means(fs_mat)[step_0_offset:step_0_offset+curve_len_8]\n",
    "    \n",
    "#     fsel_mat = np.array(v[\"fact_selectivity_matrix\"])[0]\n",
    "#     f_sel_diags = diag_means(fsel_mat)[step_0_offset:step_0_offset+curve_len_8]\n",
    "    \n",
    "#     metric_values = np.mean(np.vstack([f_acc_diags, f_spec_diags, f_sel_diags]), 0)\n",
    "    \n",
    "#     ax_right.plot(x_line, metric_values * 100, \"-o\", label=model_name, \n",
    "#                   color=model_colors[model_name], linewidth=2, markersize=6)\n",
    "\n",
    "# # Plot Base model last with dashed line\n",
    "# k_base = [key for key, val in data.items() if val[\"model_name\"] == \"Base\"][0]\n",
    "# v_base = data[k_base]\n",
    "\n",
    "# fa_mat = np.array(v_base[\"fact_accuracy_matrix\"])[0]\n",
    "# mat_size = fa_mat.shape[0]\n",
    "# step_0_offset = mat_size - 1\n",
    "# f_acc_diags = diag_means(fa_mat)[step_0_offset:step_0_offset+curve_len_8]\n",
    "\n",
    "# fs_mat = np.array(v_base[\"fact_specificity_matrix\"])[0]\n",
    "# f_spec_diags = diag_means(fs_mat)[step_0_offset:step_0_offset+curve_len_8]\n",
    "\n",
    "# fsel_mat = np.array(v_base[\"fact_selectivity_matrix\"])[0]\n",
    "# f_sel_diags = diag_means(fsel_mat)[step_0_offset:step_0_offset+curve_len_8]\n",
    "# metric_values_base = np.mean(np.vstack([f_acc_diags, f_spec_diags, f_sel_diags]), 0)\n",
    "\n",
    "# ax_right.plot(x_line, metric_values_base * 100, \"--o\", label=\"Base\", \n",
    "#               color='black', linewidth=2, markersize=6, dashes=(5, 2))\n",
    "\n",
    "# ax_right.set_title(\"Score Over Time\", fontsize=16)\n",
    "# ax_right.set_xticks(list(x_line))\n",
    "# ax_right.set_xlabel(\"Steps Since Seen\", fontsize=14)\n",
    "# ax_right.set_ylim(50, 100)\n",
    "# ax_right.set_ylabel(\"Score (%)\", fontsize=14)\n",
    "# ax_right.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# # Single legend for the entire figure\n",
    "# handles, labels = ax_left.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(0.16, -0.04), ncol=5, fontsize=16)\n",
    "\n",
    "# plt.savefig(\"../plots/warmup_combined.png\", dpi=600, bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edab78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a figure with 4 line charts showing performance over time\n",
    "# curve_len = 8\n",
    "# metrics = ['score', 'accuracy', 'specificity', 'selectivity']\n",
    "# metric_titles = ['Score', 'Accuracy', 'Specificity', 'Selectivity']\n",
    "\n",
    "# fig, axs = plt.subplots(1, len(metrics), figsize=(13, 3.5))\n",
    "\n",
    "# x = np.arange(curve_len)\n",
    "\n",
    "# # Calculate metrics for all time steps\n",
    "# for metric_idx, metric_name in enumerate(metrics):\n",
    "#     for k, v in data.items():\n",
    "#         # Calculate diagonal means for each metric\n",
    "#         if metric_name == 'score':\n",
    "#             # Score is average of the three metrics\n",
    "#             fa_mat = np.array(v[\"fact_accuracy_matrix\"])[0][:len_rollout, :len_rollout]\n",
    "#             f_acc_diags = diag_means(fa_mat)[step_0:][:curve_len]\n",
    "            \n",
    "#             fs_mat = np.array(v[\"fact_specificity_matrix\"])[0][:len_rollout, :len_rollout]\n",
    "#             f_spec_diags = diag_means(fs_mat)[step_0:][:curve_len]\n",
    "            \n",
    "#             fs_mat = np.array(v[\"fact_selectivity_matrix\"])[0][:len_rollout, :len_rollout]\n",
    "#             f_sel_diags = diag_means(fs_mat)[step_0:][:curve_len]\n",
    "            \n",
    "#             metric_values = np.mean(np.vstack([f_acc_diags, f_spec_diags, f_sel_diags]), 0)\n",
    "#         elif metric_name == 'accuracy':\n",
    "#             fa_mat = np.array(v[\"fact_accuracy_matrix\"])[0][:len_rollout, :len_rollout]\n",
    "#             metric_values = diag_means(fa_mat)[step_0:][:curve_len]\n",
    "#         elif metric_name == 'specificity':\n",
    "#             fs_mat = np.array(v[\"fact_specificity_matrix\"])[0][:len_rollout, :len_rollout]\n",
    "#             metric_values = diag_means(fs_mat)[step_0:][:curve_len]\n",
    "#         elif metric_name == 'selectivity':\n",
    "#             fs_mat = np.array(v[\"fact_selectivity_matrix\"])[0][:len_rollout, :len_rollout]\n",
    "#             metric_values = diag_means(fs_mat)[step_0:][:curve_len]\n",
    "        \n",
    "#         # Plot the line\n",
    "#         axs[metric_idx].plot(x, metric_values * 100, \"--o\", label=v[\"model_name\"], color=model_colors[v[\"model_name\"]])\n",
    "    \n",
    "#     axs[metric_idx].set_title(metric_titles[metric_idx], fontsize=14)\n",
    "#     axs[metric_idx].set_xticks(list(x))\n",
    "#     axs[metric_idx].set_xlabel(\"Steps Since Seen\", fontsize=14)\n",
    "#     axs[metric_idx].set_ylim(0, 100)\n",
    "\n",
    "# # Add legend to last subplot\n",
    "# axs[-1].legend(fontsize=12, loc=\"lower left\")\n",
    "\n",
    "# # Set y-label on first subplot\n",
    "# axs[0].set_ylabel(\"% Performance\", fontsize=14)\n",
    "# fig.suptitle(\"Memory Retention Over Time\", y=0.94, fontsize=16)\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "# plt.savefig(\"../plots/warmup_bar_by_time.png\", dpi=600, bbox_inches=\"tight\")\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
